# Robot Framework Expert Fine-Tune Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

End-to-end, reproducible pipeline to build a Robot Framework + Python dataset, validate it, and generate a Colab notebook to fine-tune a small instruct model with Unsloth QLoRA. The workflow is designed to be lightweight locally and GPU-heavy only in Colab.

## What this repo does

- Build a curated dataset from local Robot Framework docs plus synthetic examples
- Validate dataset schema and basic quality checks
- Generate a Colab notebook tailored to this repo
- Fine-tune with QLoRA and optionally merge adapters
- Publish dataset and artifacts to Hugging Face

## Quick start

1. Create a local `.env` (do not commit) from the template:

```bash
cp .env.example .env
```

2. Set tokens as needed:

- `HF_TOKEN` is required to publish datasets and model artifacts to Hugging Face
- `GH_TOKEN` is only needed if you want the helper to create or re-point the GitHub repo

3. (Optional) Add official Robot Framework docs under `sources/robotframework_docs/`.

4. Run local preparation:

```bash
./run_all.sh
```

On Windows:

```powershell
./run_all.ps1
```

The script builds and validates the dataset, optionally publishes it to Hugging Face, then prints a Colab URL. Open the URL and click **Run all** in Colab.

## Configuration

Project defaults live in `config/config.yaml`:

- `base_model` and `fallback_model`
- `hf_model_repo` and `hf_dataset_repo`
- `seed`, `max_seq_length`, LoRA hyperparameters
- dataset sizing: `synthetic_count`, `eval_size`

Token behavior:

- `HF_TOKEN` is required for dataset publish (`tools/publish_dataset.py`) and model upload during training
- `GH_TOKEN` is used by `tools/github_setup.py` to create a repo and set `config/config.yaml` if no remote exists

## Training and evaluation

Training happens in Colab via `colab/finetune_unsloth.ipynb` (generated by the prep step). The notebook runs `train/train_unsloth.py`, which:

- Tries `base_model` and falls back on OOM or CUDA errors
- Saves adapter and optional merged model
- Generates a lightweight eval report with `tools/make_eval_report.py`
- Prepares a Hugging Face upload folder with a model card

### Local training (optional)

If you have a compatible GPU locally, you can run training outside Colab:

```bash
pip install -r requirements-train.txt
python train/train_unsloth.py --dataset dataset --output-dir outputs
```

Note: `HF_TOKEN` is required because the training script publishes artifacts to Hugging Face.

## Outputs

Local prep creates:

- `dataset/train.jsonl` and `dataset/eval.jsonl`
- `dataset/README.md` and `dataset/build_meta.json`
- `colab/finetune_unsloth.ipynb`

Training creates:

- `outputs/adapter/` and optional `outputs/merged/`
- `outputs/run_meta.json` and `outputs/training_config.json`
- `eval/` report artifacts

## Repository layout

- `tools/` dataset build, validation, publishing, and eval utilities
- `train/` Unsloth training entrypoint
- `colab/` notebook template and generated notebook
- `data/` synthetic examples
- `config/` defaults

## Contributing

Contributions are welcome. Please see `CONTRIBUTING.md` for setup details and the recommended workflow. For security issues, see `SECURITY.md`.

## License

MIT. See `LICENSE`.
