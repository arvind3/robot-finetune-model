# Robot Framework Expert Fine-Tune Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

End-to-end, reproducible pipeline to build a Robot Framework + Python dataset, validate it, and generate a Colab notebook to fine-tune a small instruct model with Unsloth QLoRA. The workflow is designed to be lightweight locally and GPU-heavy only in Colab.

## What this repo does

- Build a curated dataset from local Robot Framework docs plus synthetic examples
- Validate dataset schema and basic quality checks
- Generate a Colab notebook tailored to this repo
- Fine-tune with QLoRA and optionally merge adapters
- Publish dataset and artifacts to Hugging Face

## Quick start

1. Create a local `.env` (do not commit) from the template:

```bash
cp .env.example .env
```

2. Set tokens as needed:

- `HF_TOKEN` is required to publish datasets and model artifacts to Hugging Face
- `GH_TOKEN` is only needed if you want the helper to create or re-point the GitHub repo

3. Ensure authoritative Robot Framework docs are present under `sources/robotframework_docs/`.
   The repo includes seeded `official_*` `.md`/`.txt`/`.html` files, and `tools/build_dataset.py` will
   auto-fetch from `robotframework.org` if the folder is empty.

4. Run local preparation:

```bash
./run_all.sh
```

On Windows:

```powershell
./run_all.ps1
```

The script builds and validates the dataset, optionally publishes it to Hugging Face, then prints a Colab URL. Open the URL and click **Run all** in Colab.

## Fully Automated Colab Run (Optional)

There is no stable public Colab API for this flow, so automation is implemented using Playwright browser control plus Hugging Face artifact polling.

1. Install optional automation dependency:

```bash
pip install -r requirements-automation.txt
python -m playwright install chromium
```

2. Enable autorun and execute prep:

```bash
COLAB_AUTORUN=1 ./run_all.sh
```

On Windows PowerShell:

```powershell
$env:COLAB_AUTORUN = "1"
./run_all.ps1
```

Behavior:

- On first run, a browser profile opens; sign in to Google once and reuse the same profile directory.
- Opens Colab notebook and triggers **Run all**
- Waits for a new Hugging Face model commit from this run
- Downloads eval artifacts (or `run_artifacts.zip` fallback) to `artifacts/colab_run/`
- Generates leadership brief at `artifacts/colab_run/leadership/brief.md`

Direct invocation (if you already have notebook URL):

```bash
python tools/autorun_colab_playwright.py --colab-url "<your colab url>"
```

By default, `run_all` installs Playwright + Chromium when `COLAB_AUTORUN=1`. Set `COLAB_AUTORUN_SKIP_SETUP=1` to skip that setup step.

Useful env vars are documented in `.env.example` (`COLAB_AUTORUN_*`, `COLAB_USER_DATA_DIR`).

## Configuration

Project defaults live in `config/config.yaml`:

- `base_model` and `fallback_model`
- `hf_model_repo` and `hf_dataset_repo`
- `seed`, `max_seq_length`, LoRA hyperparameters
- dataset sizing: `synthetic_count`, `eval_size`
- comparative eval suite and gating:
  - `eval_suite_path`, `eval_max_samples`
  - `eval_gate_mode` (`warn` by default)
  - improvement/drop thresholds for robot-table, instruction-following, and disclaimer behavior
  - `eval_require_full_metrics` (`true` by default; blocks legacy partial-metric runs for leadership reporting)
- documentation ingestion controls:
  - `auto_fetch_official_docs` (`true` by default; fetch docs when source folder is empty)
  - `require_doc_sources` (`true` by default; fail build if doc sources cannot be obtained)
  - `official_doc_urls` (override authoritative source URLs)

Token behavior:

- `HF_TOKEN` is required for dataset publish (`tools/publish_dataset.py`) and model upload during training
- `GH_TOKEN` is used by `tools/github_setup.py` to create a repo and set `config/config.yaml` if no remote exists

## Training and evaluation

Training happens in Colab via `colab/finetune_unsloth.ipynb` (generated by the prep step). The notebook runs `train/train_unsloth.py`, which:

- Tries `base_model` and falls back on OOM or CUDA errors
- Saves adapter and optional merged model
- Runs comparative evaluation (base vs fine-tuned) via `tools/run_eval_compat.py` when available, and falls back to `tools/make_eval_report.py` otherwise
- Prepares a Hugging Face upload folder with a model card

### Local training (optional)

If you have a compatible GPU locally, you can run training outside Colab:

```bash
pip install -r requirements-train.txt
python train/train_unsloth.py --dataset dataset --output-dir outputs
```

Note: `HF_TOKEN` is required because the training script publishes artifacts to Hugging Face.

## Model Effectiveness Evaluation

The comparative evaluator runs both base and fine-tuned models on a curated suite (`data/eval_suite_v1.jsonl`) and writes side-by-side artifacts under `eval/`:

- `eval/base/predictions.jsonl`
- `eval/finetuned/predictions.jsonl`
- `eval/base_metrics.json`
- `eval/finetuned_metrics.json`
- `eval/comparison_metrics.json`
- `eval/status.json`
- `eval/report.md`

`eval/report.md` includes an executive summary, industry-impact view, and qualitative side-by-side samples.

Run it directly with:

```bash
python tools/make_eval_report.py \
  --base-model Qwen/Qwen2.5-3B-Instruct \
  --adapter-dir outputs/adapter \
  --merged-dir outputs/merged \
  --eval-suite data/eval_suite_v1.jsonl \
  --out-dir eval
```

By default, gating is soft (`warn`): threshold misses are reported in `eval/status.json` without failing the command.

## How to Use the Fine-Tuned Model

See the dedicated usage tutorial: `docs/using-finetuned-lora.md`.

## Outputs

Local prep creates:

- `dataset/train.jsonl` and `dataset/eval.jsonl`
- `dataset/README.md` and `dataset/build_meta.json`
- `colab/finetune_unsloth.ipynb`

Training creates:

- `outputs/adapter/` and optional `outputs/merged/`
- `outputs/run_meta.json` and `outputs/training_config.json`
- `outputs/run_artifacts.zip` (portable bundle: eval + run metadata)
- `eval/` comparative report artifacts (base vs fine-tuned, deltas, gate status)

## Repository layout

- `tools/` dataset build, validation, publishing, and eval utilities
- `train/` Unsloth training entrypoint
- `colab/` notebook template and generated notebook
- `data/` synthetic examples
- `config/` defaults
- `requirements-automation.txt` optional Playwright dependency for Colab autorun

## Contributing

Contributions are welcome. Please see `CONTRIBUTING.md` for setup details and the recommended workflow. For security issues, see `SECURITY.md`.

## License

MIT. See `LICENSE`.
